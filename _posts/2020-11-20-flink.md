---
layout: post
section-type: post
title: "flink란?"
category: flink
tags: [ 'flink' ]
comments: true
---
제 블로그의 모든 글은 IMHO로 쓴 것입니다.
서로에 대한 존중을 담은 덧글을 남겨 소통을 하신다면 더 좋은 글로 발전이 될 수 있을 것 같습니다.
존중이 담기지 않은 덧글은 언제든 삭제될 수 있습니다.
감사합니다:)  
---  

# flink
- 스트림 처리 프레임 워크
- 데이터 처리 시스템이며 Hadoop의 MapReduce 구성 요소의 대안 입니다. MapReduce 위에 구축하지 않고 자체 런타임을 제공
- 무한 데이터 세트를 염두에두고 설계된 데이터 처리 엔진 유형
- 데이터가 작업의 시작과 끝으로 바인드되고 유한 데이터를 처리 한 후 작업이 완료되는 배치 처리와 달리 스트리밍은 무제한 데이터를 연속적으로 계속해서 처리하기위한 것


# 빌딩블록
flink = 소스 + 변환 + 싱크

- 데이터 소스 : Flink에서 처리하는 수신 데이터
- 변환 : 처리 단계, Flink가 들어오는 데이터를 수정하는 경우
- 데이터 싱크 : Flink가 처리 후 데이터를 보내는 곳

소스와 싱크는 database, kafka, file등이 될 수 있다.

# Flink의 추상화 레벨

- Stateful Stream Processing : 사용자가 직접 state, time 등을 관리할 수 있는 low-level 이 위치합니다.
- DataStream / DataSet API : 핵심적으로 가장 많이 사용하는 Core API 가 위치합니다.
- Table API : Library로 제공되는 Table API가 제공됩니다.
- SQL : select, join, aggregate 등의 고차원 함수를 사용할 수 있으며 이러한 SQL를 사용할 수 있는 High-level Language를 지원합니다.


# APIs
Flink의 경우 stream과 batch processing두가지 다 제공됩니다.   
크게 아래와 같이 세개의 APIs들을 제공합니다.   


- DataStream API: stream processing에 도움을 준다. 그냥 주는 것이 아니라 (filters, time-windows, aggregations) 등 가공을 할 수 있다.
- DataSet API: data set들을 제공해주는 batch processing 
- Table API: 마치 sql처럼 표현을 할 수있다. DATASet, DataStream 전부에 들어있다,  

간단한 소스 예제
``` scala
// Source
DataStream<String> lines = env.addSource(new FlinkKafkaConsumer<>(...));

// Transformation
DataStream<Event> events = lines.map((line) -> parse(line));

// Transformation
DataStream<Statistics> stats = events
          .keyBy("id")
          .timeWindow(Time.seconds(10))
          .apply(new MyWindowAggregationFunction());

// Sink
stat.addSink(new RollingSink(path));
```

---

참고: https://riptutorial.com/ko/apache-flink
